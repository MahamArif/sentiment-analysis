# -*- coding: utf-8 -*-
"""Naive Bayes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zA9Vn2E-axFlMD6FM79qYHslrGK05S78
"""

import re
import numpy as np
import pandas as pd
import nltk

# Plot libraries
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, roc_auc_score, recall_score, f1_score, balanced_accuracy_score

import nltk
from nltk.corpus import stopwords
from nltk.tokenize.toktok import ToktokTokenizer

nltk.download('stopwords')
stop = set(stopwords.words('english'))
stopword_list=nltk.corpus.stopwords.words('english')
tokenizer = ToktokTokenizer()

DATASET_COLUMNS  = ["sentiment", "ids", "date", "flag", "user", "text"]
DATASET_ENCODING = "ISO-8859-1"
dataset = pd.read_csv('./training.1600000.processed.noemoticon.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)
dataset.head()

dataset = dataset[['sentiment','text']]
dataset.head()

# Replacing the values.
dataset['sentiment'] = dataset['sentiment'].replace(4,1)
dataset.head()

dataset['sentiment'].value_counts()

contractions = pd.read_csv('./contractions.csv', index_col='Contraction')
contractions.index = contractions.index.str.lower()
contractions.Meaning = contractions.Meaning.str.lower()
contractions_dict = contractions.to_dict()['Meaning']

# Defining regex patterns.
urlPattern        = r"((http://)[^ ]*|(https://)[^ ]*|(www\.)[^ ]*)"
userPattern       = '@[^\s]+'
hashtagPattern    = '#[^\s]+'
alphaPattern      = "[^a-z0-9<>]"
sequencePattern   = r"(.)\1\1+"
seqReplacePattern = r"\1\1"

# Defining regex for emojis
smileemoji        = r"[8:=;]['`\-]?[)d]+"
sademoji          = r"[8:=;]['`\-]?\(+"
neutralemoji      = r"[8:=;]['`\-]?[\/|l*]"
lolemoji          = r"[8:=;]['`\-]?p+"

from bs4 import BeautifulSoup
def strip_html(text):
    soup = BeautifulSoup(text, "html.parser")
    return soup.get_text()

def preprocess_apply(tweet):
    tweet = tweet.lower()

    # Replace all URls with '<url>'
    tweet = re.sub(urlPattern,'<url>',tweet)
    # Replace @USERNAME to '<user>'.
    tweet = re.sub(userPattern,'<user>', tweet)
    
    # Replace 3 or more consecutive letters by 2 letter.
    tweet = re.sub(sequencePattern, seqReplacePattern, tweet)

    # Replace all emojis.
    tweet = re.sub(r'<3', '<heart>', tweet)
    tweet = re.sub(smileemoji, '<smile>', tweet)
    tweet = re.sub(sademoji, '<sadface>', tweet)
    tweet = re.sub(neutralemoji, '<neutralface>', tweet)
    tweet = re.sub(lolemoji, '<lolface>', tweet)

    for contraction, replacement in contractions_dict.items():
        tweet = tweet.replace(contraction, replacement)
        
    # Remove non-alphanumeric and symbols
    tweet = re.sub(alphaPattern, ' ', tweet)

    # Adding space on either side of '/' to seperate words (After replacing URLS).
    tweet = re.sub(r'/', ' / ', tweet)

    tweet = strip_html(tweet)

    stemmer = nltk.porter.PorterStemmer()
    tweet = ' '.join([stemmer.stem(word) for word in tweet.split()])
    
    tokens = tokenizer.tokenize(tweet)
    tokens = [token.strip() for token in tokens]
    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]
    filtered_text = ' '.join(filtered_tokens)    
    return filtered_text

dataset['processed_text'] = dataset.text.apply(preprocess_apply)

count=0
for row in dataset.itertuples():
    print("Text:", row[2])
    print("Processed:", row[3])
    count+=1
    if count>10:
        break

print("number of rows of data {}".format(dataset.shape[0]))

X_data, y_data = np.array(dataset['processed_text']), np.array(dataset['sentiment'])
X_train, X_test, y_train, y_test = train_test_split(X_data, y_data, test_size = 0.08, random_state = 42)
print('Data Split done.')

# Bag of Words Model
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

cv = CountVectorizer(min_df=0, max_df=1, binary=False, ngram_range=(1,3))
cv_train_reviews = cv.fit_transform(X_train)
cv_test_reviews=cv.transform(X_test)

print('BOW_cv_train:',cv_train_reviews.shape)
print('BOW_cv_test:',cv_test_reviews.shape)

# TF-IDF Vector
tv = TfidfVectorizer(min_df=0, max_df=1, use_idf=True, ngram_range=(1,3))
tv_train_reviews=tv.fit_transform(X_train)
tv_test_reviews=tv.transform(X_test)

print('Tfidf_train:',tv_train_reviews.shape)
print('Tfidf_test:',tv_test_reviews.shape)

from sklearn.naive_bayes import MultinomialNB

mnb = MultinomialNB()

#fitting the svm for bag of words
mnb_bow = mnb.fit(cv_train_reviews, y_train)
print(mnb_bow)

#fitting the svm for tfidf features
mnb_tfidf = mnb.fit(tv_train_reviews, y_train)
print(mnb_tfidf)

#Predicting the model for bag of words
mnb_bow_predict = mnb.predict(cv_test_reviews)
print(mnb_bow_predict)

#Predicting the model for tfidf features
mnb_tfidf_predict = mnb.predict(tv_test_reviews)
print(mnb_tfidf_predict)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, roc_auc_score, recall_score, f1_score, balanced_accuracy_score

#Accuracy score for bag of words
mnb_bow_score = accuracy_score(y_test, mnb_bow_predict)
print("mnb_bow_score :",mnb_bow_score)
#Accuracy score for tfidf features
mnb_tfidf_score = accuracy_score(y_test, mnb_tfidf_predict)
print("mnb_tfidf_score :",mnb_tfidf_score)

#Classification report for tfidf features
mnb_tfidf_report = classification_report(y_test, mnb_tfidf_predict)
print(mnb_tfidf_report)

auc = roc_auc_score(y_test, mnb_tfidf_predict)
prec = precision_score(y_test, mnb_tfidf_predict)
rec = recall_score(y_test, mnb_tfidf_predict)
f1 = f1_score(y_test, mnb_tfidf_predict)
    
print('auc :{}'.format(auc))
print('precision :{}'.format(prec))
print('recall :{}'.format(rec))
print('f1 :{}'.format(f1))

balanced_accuracy = balanced_accuracy_score(y_test, mnb_tfidf_predict)
print('balanced_accuracy :{}'.format(balanced_accuracy))
